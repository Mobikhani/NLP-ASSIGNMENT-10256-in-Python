{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NAME: **MOHAIB KHAN**\n",
        "\n",
        "CLASS: **BSSE-7TH-B**\n",
        "\n",
        "ROLL NO: **10256**\n",
        "\n",
        "SUBMITTION DATE: **30/10/2024**"
      ],
      "metadata": {
        "id": "CgPP85g4LaNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QUESTION NO 01**"
      ],
      "metadata": {
        "id": "xPRheF43Jnvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = df.columns.tolist()\n",
        "print(column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "458SqrogFbXm",
        "outputId": "4007dae6-7c8f-4d85-e789-e7d7d0cf4bbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abstract', 'author', 'date', 'pdf_url', 'title', 'pdf_text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Set Up Google Drive Access and Load Corpus**"
      ],
      "metadata": {
        "id": "anLvj-4zHn_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the corpus from CSV, focusing on the 'abstract' or 'pdf_text' column\n",
        "corpus_file_path = '/content/drive/MyDrive/arxiv_papers.csv'\n",
        "df = pd.read_csv(corpus_file_path)\n",
        "\n",
        "# Combine all text data from the 'abstract' column; change to 'pdf_text' if needed\n",
        "data = \" \".join(df['abstract'].dropna().tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2PYt_K9JF7yw",
        "outputId": "5ab553b1-21be-48ab-b5f3-234991cbd6e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Text Preprocessing**\n",
        "\n",
        "**This code performs text cleaning, tokenization, stopword removal, and lemmatization.**"
      ],
      "metadata": {
        "id": "jvFJjVcCHdrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Text Cleaning\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Clean the data\n",
        "cleaned_data = clean_text(data)\n",
        "print(\"Cleaned Data (First 500 characters):\", cleaned_data[:500])  # Preview of cleaned text\n",
        "\n",
        "# Tokenization and Stopword Removal\n",
        "tokens = nltk.word_tokenize(cleaned_data)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "print(\"Tokens after Stopword Removal (First 20 tokens):\", filtered_tokens[:20])  # Preview of tokens\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "print(\"Lemmatized Tokens (First 20 tokens):\", lemmatized_tokens[:20])  # Preview of lemmatized tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wiKocHx3GmQs",
        "outputId": "e8ab0768-6310-421d-bb9a-3abfd6e69d68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Data (First 500 characters): we first present our view of detection and correction of syntactic errors we then introduce a new correction method based on heuristic criteria used to decide which correction should be preferred weighting of these criteria leads to a flexible and parametrable system which can adapt itself to the user a partitioning of the trees based on linguistic criteria agreement rules rather than computational criteria is then necessary we end by proposing extensions to lexical correction and to some syntac\n",
            "Tokens after Stopword Removal (First 20 tokens): ['first', 'present', 'view', 'detection', 'correction', 'syntactic', 'errors', 'introduce', 'new', 'correction', 'method', 'based', 'heuristic', 'criteria', 'used', 'decide', 'correction', 'preferred', 'weighting', 'criteria']\n",
            "Lemmatized Tokens (First 20 tokens): ['first', 'present', 'view', 'detection', 'correction', 'syntactic', 'error', 'introduce', 'new', 'correction', 'method', 'based', 'heuristic', 'criterion', 'used', 'decide', 'correction', 'preferred', 'weighting', 'criterion']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Word Embedding**\n",
        "\n",
        "**Using Word2Vec from the gensim library to create word embeddings.**"
      ],
      "metadata": {
        "id": "8wKJHWOhHaqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Create Word2Vec model\n",
        "model = Word2Vec([lemmatized_tokens], vector_size=100, window=5, min_count=2, sg=1)\n",
        "word_vectors = model.wv\n",
        "\n",
        "# Example: Get vector for a specific word (adjust word as needed)\n",
        "print(\"Vector for 'research':\", word_vectors['research'] if 'research' in word_vectors else \"Word not in vocabulary\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a1KeagWTHQsb",
        "outputId": "14923a04-b2c4-4cfc-d867-9e247b49d9cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'research': [-0.09597938  0.04817301  0.03520697 -0.18883356  0.00452412 -0.30815223\n",
            "  0.10842972  0.3195855  -0.09427086 -0.06864997 -0.04312519 -0.1758261\n",
            " -0.14962809  0.04688559  0.02893239 -0.05368261 -0.02925871 -0.11721992\n",
            "  0.02723616 -0.19980678  0.05839466  0.06323639  0.1160235  -0.12231019\n",
            " -0.07332232  0.05454352 -0.14621918 -0.07184091 -0.07285503 -0.04595394\n",
            "  0.14506754  0.00372419  0.11108604 -0.13494326 -0.05047325  0.10980233\n",
            "  0.13286512 -0.07249369 -0.16698632 -0.29481724  0.00372281 -0.17214891\n",
            "  0.00435141 -0.02077201  0.1601517  -0.08331051 -0.1203436  -0.01680325\n",
            "  0.14170131  0.13401279  0.14013405 -0.17045675 -0.0611458  -0.06087408\n",
            " -0.07678086  0.0385175   0.07323485  0.00218575 -0.19613388  0.05558421\n",
            "  0.04570123  0.07914662 -0.05761265 -0.05554193 -0.13129601  0.18643789\n",
            "  0.04180723  0.13608855 -0.09052119  0.17414398 -0.1673439   0.07500046\n",
            "  0.14404681  0.00767872  0.19879793  0.05922469  0.05944034 -0.09374236\n",
            " -0.13121538 -0.00213671 -0.06244595  0.06857093 -0.08367636  0.19370253\n",
            "  0.01648789  0.00504938 -0.04659484  0.13267127  0.14988738  0.07116204\n",
            "  0.11580476  0.04536944  0.01668226 -0.00685782  0.29491463  0.08239824\n",
            "  0.11483156 -0.13752232  0.02725036 -0.03755618]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Encoding Techniques**\n",
        "\n",
        "**Bag of Words and One-Hot Encoding **"
      ],
      "metadata": {
        "id": "2NddcB25H-3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Bag of Words Encoding\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform([' '.join(lemmatized_tokens)])\n",
        "\n",
        "print(\"Bag of Words Encoding (First 10 words):\", bow_matrix.toarray()[:20])  # Preview of Bag of Words\n",
        "\n",
        "# One-Hot Encoding (limited sample for display purposes)\n",
        "lb = LabelBinarizer()\n",
        "one_hot_encoded = lb.fit_transform(lemmatized_tokens[:20])\n",
        "print(\"One-Hot Encoding (First 20 words):\", one_hot_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pobeUQPpH9T7",
        "outputId": "6f798a2b-b743-4cd6-cdd6-43620b5d4fd5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Encoding (First 10 words): [[1 1 1 ... 1 1 1]]\n",
            "One-Hot Encoding (First 20 words): [[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Parts of Speech (POS) Tagging**"
      ],
      "metadata": {
        "id": "dzHYti1RIezJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Sample lemmatized tokens (replace with your actual lemmatized tokens)\n",
        "lemmatized_tokens = [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \".\"]\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Increase the maximum length limit\n",
        "nlp.max_length = len(' '.join(lemmatized_tokens)) + 100 # add a buffer\n",
        "\n",
        "# Process the text in smaller chunks if still too large\n",
        "chunk_size = 1000000  # Adjust this based on your available memory\n",
        "pos_tags = []\n",
        "for i in range(0, len(lemmatized_tokens), chunk_size):\n",
        "    chunk = lemmatized_tokens[i : i + chunk_size]\n",
        "    doc = nlp(' '.join(chunk))\n",
        "    pos_tags.extend([(token.text, token.pos_) for token in doc])\n",
        "\n",
        "# Print the first 20 POS tags\n",
        "print(\"POS Tags (First 20 tokens):\", pos_tags[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "57xgyHdvJXZ9",
        "outputId": "646b2cc6-0092-45a8-9304-c863a0ec17da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags (First 20 tokens): [('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('sample', 'NOUN'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QUESTION NO 02:**"
      ],
      "metadata": {
        "id": "BC28UxvCJecm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Sentiment Analysis (using TextBlob)**"
      ],
      "metadata": {
        "id": "y0DNUjK8JmJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Sentiment Analysis using TextBlob\n",
        "from textblob import TextBlob\n",
        "\n",
        "# TextBlob Sentiment Analysis\n",
        "textblob_analysis = TextBlob(data)\n",
        "textblob_sentiment = textblob_analysis.sentiment\n",
        "print(\"TextBlob Sentiment Analysis Scores:\")\n",
        "print(\"Polarity:\", textblob_sentiment.polarity)  # Range from -1 (negative) to 1 (positive)\n",
        "print(\"Subjectivity:\", textblob_sentiment.subjectivity)  # Range from 0 (objective) to 1 (subjective)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MJzS6iaoUAZc",
        "outputId": "e4f6991e-a81d-4470-b37a-edbecdf3f6c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextBlob Sentiment Analysis Scores:\n",
            "Polarity: 0.09946322238259706\n",
            "Subjectivity: 0.4222628006418276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2: Sentiment Analysis using VADER"
      ],
      "metadata": {
        "id": "eaWeD61rVxHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "subset_data = \" \".join(df['abstract'].dropna().tolist()[:300])\n",
        "\n",
        "# VADER Sentiment Analysis\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "vader_scores = analyzer.polarity_scores(subset_data)\n",
        "print(\"VADER Sentiment Analysis Scores for Subset:\")\n",
        "print(vader_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "89a1ghSjWQk3",
        "outputId": "2d295ee7-befe-481d-e8b3-616a8d454483"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VADER Sentiment Analysis Scores for Subset:\n",
            "{'neg': 0.036, 'neu': 0.862, 'pos': 0.102, 'compound': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Classification (Naive Bayes)**"
      ],
      "metadata": {
        "id": "V_HsMhmhYOpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "IFiG8DNbKfXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XQjr0r2cYHpa",
        "outputId": "3059b06f-33cd-4f6e-c469-13df10cfb74f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3330429732868757\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Computer Science       0.34      0.34      0.34      1160\n",
            "     Mathematics       0.34      0.33      0.33      1166\n",
            "         Physics       0.32      0.33      0.33      1118\n",
            "\n",
            "        accuracy                           0.33      3444\n",
            "       macro avg       0.33      0.33      0.33      3444\n",
            "    weighted avg       0.33      0.33      0.33      3444\n",
            "\n"
          ]
        }
      ]
    }
  ]
}